"""
# MLOps íŒŒì´í”„ë¼ì¸ ì˜ˆì œ - ë‹¤ì¤‘ ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ ì„ íƒ

## í”„ë¡œì íŠ¸ ì •ë³´
- **DAG ID**: mlops_training_pipeline
- **ì‘ì„±ì**: eric.cho@robos.one
- **ì‘ì„±ì¼**: 2025-08-24
- **ëª©ì **: MinIO ë‹¤ì¤‘ ë°ì´í„°ì…‹ê³¼ MLflowë¥¼ í™œìš©í•œ ë™ì  ML í›ˆë ¨ íŒŒì´í”„ë¼ì¸

## ê°œìš”
1. MinIOì—ì„œ ì„ íƒëœ ë°ì´í„°ì…‹ ë¡œë“œ
2. íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ íƒ€ì…ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì „ë‹¬
3. MLflowë¡œ ëª¨ë¸ í›ˆë ¨ ë° ì‹¤í—˜ ì¶”ì 
4. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë“±ë¡

## ì›Œí¬í”Œë¡œìš°
```
Dataset Selection â†’ Data Load â†’ Feature Engineering â†’ Model Training â†’ Evaluation â†’ Model Registry
```

## ì§€ì› ë°ì´í„°ì…‹
- iris: ë¶“ê½ƒ ë¶„ë¥˜ ë°ì´í„°
- titanic: íƒ€ì´íƒ€ë‹‰ ìƒì¡´ ì˜ˆì¸¡
- housing: ì£¼íƒ ê°€ê²© ì˜ˆì¸¡

## ì§€ì› ëª¨ë¸
- xgboost: XGBoost ë¶„ë¥˜/íšŒê·€
- lightgbm: LightGBM ë¶„ë¥˜/íšŒê·€
- sklearn: Scikit-learn ëª¨ë¸ë“¤
"""
import datetime
import pendulum
import json
from typing import Dict, Any

from airflow.sdk import dag, task, Asset
from airflow.models.param import Param

# MinIO ë°ì´í„°ì…‹ Assets
datasets_assets = {
    "iris": Asset(
        uri="s3://ml-bucket/datasets/iris.csv",
        name="iris_dataset",
        extra={"type": "classification", "features": 4, "classes": 3}
    ),
    "titanic": Asset(
        uri="s3://ml-bucket/datasets/titanic.csv", 
        name="titanic_dataset",
        extra={"type": "classification", "features": 8, "classes": 2}
    ),
    "housing": Asset(
        uri="s3://ml-bucket/datasets/housing.csv",
        name="housing_dataset", 
        extra={"type": "regression", "features": 13, "target": "price"}
    )
}

# í›ˆë ¨ëœ ëª¨ë¸ Asset
model_asset = Asset(
    uri="s3://ml-bucket/models/trained_model",
    name="trained_ml_model",
    extra={"registry": "mlflow", "stage": "staging"}
)

@dag(
    dag_id="mlops_training_pipeline",
    schedule="@manual",  # ìˆ˜ë™ ì‹¤í–‰ (íŒŒë¼ë¯¸í„° ì „ë‹¬ìš©)
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    catchup=False,
    tags=["mlops", "mlflow", "minio", "machine-learning"],
    params={
        "dataset": Param(
            default="iris",
            type="string", 
            enum=["iris", "titanic", "housing"],
            description="í›ˆë ¨ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì„ íƒ"
        ),
        "model_type": Param(
            default="xgboost",
            type="string",
            enum=["xgboost", "lightgbm", "sklearn"],
            description="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… ì„ íƒ"
        ),
        "hyperparams": Param(
            default={"n_estimators": 100, "max_depth": 6, "learning_rate": 0.1},
            type="object",
            description="ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°"
        ),
        "experiment_name": Param(
            default="ml_experiment_001",
            type="string",
            description="MLflow ì‹¤í—˜ëª…"
        )
    },
    default_args={
        "owner": "eric.cho@robos.one",
        "retries": 2,
        "retry_delay": pendulum.duration(minutes=3),
    },
    description="ë™ì  ML í›ˆë ¨ íŒŒì´í”„ë¼ì¸ - ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì„ íŒŒë¼ë¯¸í„°ë¡œ ì„ íƒ",
    doc_md=__doc__,
)
def mlops_training_pipeline():

    @task
    def validate_parameters(**context):
        """
        ì…ë ¥ëœ íŒŒë¼ë¯¸í„°ë“¤ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        params = context['params']
        dataset = params['dataset']
        model_type = params['model_type'] 
        hyperparams = params['hyperparams']
        
        print(f"ğŸ” íŒŒë¼ë¯¸í„° ê²€ì¦ ì‹œì‘")
        print(f"ğŸ“Š ì„ íƒëœ ë°ì´í„°ì…‹: {dataset}")
        print(f"ğŸ¤– ì„ íƒëœ ëª¨ë¸: {model_type}")
        print(f"âš™ï¸  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {json.dumps(hyperparams, indent=2)}")
        
        # ë°ì´í„°ì…‹ë³„ ëª¨ë¸ íƒ€ì… í˜¸í™˜ì„± ì²´í¬
        dataset_info = datasets_assets[dataset].extra
        if dataset_info['type'] == 'classification' and model_type == 'sklearn':
            print("âœ… ë¶„ë¥˜ ì‘ì—… - sklearn í˜¸í™˜")
        elif dataset_info['type'] == 'regression':
            print("âœ… íšŒê·€ ì‘ì—… í˜¸í™˜")
            
        print("âœ… ëª¨ë“  íŒŒë¼ë¯¸í„° ê²€ì¦ ì™„ë£Œ!")
        return {
            "dataset": dataset,
            "model_type": model_type, 
            "hyperparams": hyperparams,
            "dataset_info": dataset_info
        }

    @task(inlets=[datasets_assets["iris"], datasets_assets["titanic"], datasets_assets["housing"]])
    def load_dataset_from_minio(validated_params: Dict[str, Any]):
        """
        MinIOì—ì„œ ì„ íƒëœ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        dataset_name = validated_params['dataset']
        dataset_info = validated_params['dataset_info']
        
        print(f"ğŸ“¥ MinIOì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {dataset_name}")
        print(f"ğŸ—ƒï¸ ë°ì´í„°ì…‹ ì •ë³´: {dataset_info}")
        
        # ì‹¤ì œë¡œëŠ” MinIO client ì‚¬ìš©
        # from minio import Minio
        # client = Minio('localhost:9000', access_key='...', secret_key='...')
        # data = client.get_object('ml-bucket', f'datasets/{dataset_name}.csv')
        
        # ì˜ˆì œìš© ë”ë¯¸ ë°ì´í„°
        if dataset_name == "iris":
            sample_data = {
                "features": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]],
                "target": [0, 0],
                "feature_names": ["sepal_length", "sepal_width", "petal_length", "petal_width"],
                "target_names": ["setosa", "versicolor", "virginica"]
            }
        elif dataset_name == "titanic":
            sample_data = {
                "features": [[22, 1, 0, 0, 7.25], [38, 1, 1, 0, 71.28]],
                "target": [0, 1],
                "feature_names": ["age", "sex", "pclass", "sibsp", "fare"],
                "target_names": ["died", "survived"]
            }
        else:  # housing
            sample_data = {
                "features": [[0.00632, 18.0, 2.31, 0, 0.538], [0.02731, 0.0, 7.07, 0, 0.469]],
                "target": [24.0, 21.6],
                "feature_names": ["crim", "zn", "indus", "chas", "nox"],
                "target_names": ["price"]
            }
        
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ! ìƒ˜í”Œ ìˆ˜: {len(sample_data['features'])}")
        
        return {
            **validated_params,
            "data": sample_data,
            "data_path": f"s3://ml-bucket/datasets/{dataset_name}.csv"
        }

    @task
    def feature_engineering(dataset_info: Dict[str, Any]):
        """
        íŠ¹ì§• ê³µí•™ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        dataset_name = dataset_info['dataset']
        data = dataset_info['data']
        
        print(f"ğŸ”§ íŠ¹ì§• ê³µí•™ ì‹œì‘: {dataset_name}")
        
        # ê¸°ë³¸ì ì¸ ì „ì²˜ë¦¬
        processed_features = []
        for features in data['features']:
            # ì •ê·œí™” ì˜ˆì œ
            normalized = [f / max(data['features'][0]) if max(data['features'][0]) > 0 else f for f in features]
            processed_features.append(normalized)
        
        print(f"âœ… íŠ¹ì§• ê³µí•™ ì™„ë£Œ! ì²˜ë¦¬ëœ íŠ¹ì§• ìˆ˜: {len(processed_features[0])}")
        
        return {
            **dataset_info,
            "processed_features": processed_features,
            "original_features": data['features']
        }

    @task(outlets=[model_asset])
    def run_complete_mlflow_pipeline(processed_data: Dict[str, Any], **context):
        """
        MLflowê°€ í›ˆë ¨â†’í‰ê°€â†’ê²€ì¦â†’ë“±ë¡ì„ í†µí•©ìœ¼ë¡œ ìˆ˜í–‰í•˜ê³  ìµœì¢… ê²°ê³¼ë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.
        AirflowëŠ” ì´ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ë°°í¬ ê²°ì •ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        params = context['params']
        experiment_name = params['experiment_name']
        model_type = processed_data['model_type']
        hyperparams = processed_data['hyperparams']
        dataset_name = processed_data['dataset']
        
        print(f"ğŸš€ MLflow í†µí•© íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print(f"ğŸ“‹ ì‹¤í–‰ ë‹¨ê³„: í›ˆë ¨ â†’ í‰ê°€ â†’ ê²€ì¦ â†’ ë“±ë¡")
        print(f"ğŸ§ª ì‹¤í—˜ëª…: {experiment_name}")
        print(f"ğŸ¤– ëª¨ë¸ íƒ€ì…: {model_type}")
        print(f"ğŸ“Š ë°ì´í„°ì…‹: {dataset_name}")
        
        # ì‹¤ì œë¡œëŠ” ì´ëŸ° ì‹ìœ¼ë¡œ MLflow í†µí•© ì‹¤í–‰
        # import subprocess
        # result = subprocess.run([
        #     "mlflow", "run", ".",
        #     "-P", f"dataset={dataset_name}",
        #     "-P", f"model_type={model_type}",
        #     "-P", f"hyperparams={json.dumps(hyperparams)}",
        #     "-P", "run_full_pipeline=true"  # í›ˆë ¨+í‰ê°€+ê²€ì¦+ë“±ë¡ ëª¨ë‘ ì‹¤í–‰
        # ], capture_output=True, text=True)
        
        print("=" * 60)
        print("ğŸ”„ STEP 1: ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜
        if model_type == "xgboost":
            print("ğŸŒ³ XGBoost ëª¨ë¸ í›ˆë ¨")
            model_info = {"type": "XGBClassifier", "params": hyperparams}
        elif model_type == "lightgbm":
            print("ğŸ’¡ LightGBM ëª¨ë¸ í›ˆë ¨")
            model_info = {"type": "LGBMClassifier", "params": hyperparams}
        else:  # sklearn
            print("ğŸ”¬ Scikit-learn ëª¨ë¸ í›ˆë ¨")
            model_info = {"type": "RandomForestClassifier", "params": hyperparams}
        
        print("âœ… í›ˆë ¨ ì™„ë£Œ!")
        
        print("ğŸ”„ STEP 2: ëª¨ë¸ í‰ê°€ ì¤‘...")
        # í‰ê°€ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” MLflowê°€ ìˆ˜í–‰)
        accuracy = 0.85 + (hash(str(hyperparams)) % 100) / 1000
        precision = accuracy - 0.02
        recall = accuracy - 0.01
        f1_score = 2 * (precision * recall) / (precision + recall)
        
        print(f"ğŸ“Š í‰ê°€ ê²°ê³¼:")
        print(f"   - Accuracy: {accuracy:.4f}")
        print(f"   - Precision: {precision:.4f}")
        print(f"   - Recall: {recall:.4f}")
        print(f"   - F1-Score: {f1_score:.4f}")
        
        print("ğŸ”„ STEP 3: ëª¨ë¸ ê²€ì¦ ì¤‘...")
        # ê²€ì¦ ê¸°ì¤€ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ê²€ì¦ ìˆ˜í–‰)
        validation_passed = accuracy >= 0.8 and precision >= 0.78
        
        print("ğŸ”„ STEP 4: ëª¨ë¸ ë“±ë¡ ì²˜ë¦¬ ì¤‘...")
        if validation_passed:
            print("âœ… ê²€ì¦ í†µê³¼! ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡ ì¤‘...")
            registration_status = "REGISTERED"
            model_stage = "Staging"  # ë¨¼ì € Stagingì— ë“±ë¡
        else:
            print("âŒ ê²€ì¦ ì‹¤íŒ¨! ë“±ë¡í•˜ì§€ ì•ŠìŒ")
            registration_status = "REJECTED"
            model_stage = None
        
        # MLflow í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ - ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•œ ê²°ê³¼ ë¦¬í„´
        final_result = {
            # ëª¨ë¸ ì •ë³´
            "model_info": model_info,
            "model_uri": f"s3://ml-bucket/models/{dataset_name}_{model_type}",
            "experiment_name": experiment_name,
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (MLflowì—ì„œ ìˆ˜ì§‘ë¨)
            "metrics": {
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1_score": f1_score
            },
            
            # ê²€ì¦ ê²°ê³¼ (MLflowì—ì„œ ìˆ˜í–‰ë¨)
            "validation": {
                "passed": validation_passed,
                "threshold_accuracy": 0.8,
                "threshold_precision": 0.78
            },
            
            # ë“±ë¡ ê²°ê³¼ (MLflow Model Registry)
            "registration": {
                "status": registration_status,
                "stage": model_stage,
                "model_name": f"{dataset_name}_best_model"
            },
            
            # ë°°í¬ ì¶”ì²œì‚¬í•­ (MLflowê°€ íŒë‹¨)
            "deployment_recommendation": {
                "action": "deploy_to_staging" if validation_passed else "retrain_required",
                "reason": "ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡±" if validation_passed else "ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬",
                "confidence": "high" if accuracy > 0.9 else "medium" if accuracy > 0.85 else "low"
            }
        }
        
        print("=" * 60)
        print(f"ğŸ¯ MLflow í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥: {accuracy:.4f}")
        print(f"âœ… ê²€ì¦ ê²°ê³¼: {'í†µê³¼' if validation_passed else 'ì‹¤íŒ¨'}")
        print(f"ğŸ“ ë“±ë¡ ìƒíƒœ: {registration_status}")
        print(f"ğŸš€ ë°°í¬ ì¶”ì²œ: {final_result['deployment_recommendation']['action']}")
        
        return final_result

    @task
    def decide_deployment_action(mlflow_result: Dict[str, Any]):
        """
        MLflow í†µí•© íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°°í¬ ì•¡ì…˜ì„ ê²°ì •í•©ë‹ˆë‹¤.
        ì´ê²ƒì´ Airflowì˜ í•µì‹¬ ì—­í•  - ì˜ì‚¬ê²°ì •!
        """
        recommendation = mlflow_result['deployment_recommendation']
        metrics = mlflow_result['metrics']
        validation = mlflow_result['validation']
        registration = mlflow_result['registration']
        
        print(f"ğŸ¤” ë°°í¬ ê²°ì • ë¶„ì„ ì¤‘...")
        print(f"ğŸ“Š MLflow ì¶”ì²œ: {recommendation['action']}")
        print(f"ğŸ’ª ì„±ëŠ¥: {metrics['accuracy']:.4f} (ì„ê³„ê°’: {validation['threshold_accuracy']})")
        print(f"âœ… ê²€ì¦: {'í†µê³¼' if validation['passed'] else 'ì‹¤íŒ¨'}")
        
        # Airflowì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì ìš©
        if recommendation['action'] == "deploy_to_staging":
            if metrics['accuracy'] > 0.95:
                final_action = "deploy_to_production"
                print(f"ğŸš€ ê²°ì •: ì„±ëŠ¥ ìš°ìˆ˜ - ë°”ë¡œ í”„ë¡œë•ì…˜ ë°°í¬!")
            else:
                final_action = "deploy_to_staging"
                print(f"ğŸ“Š ê²°ì •: ìŠ¤í…Œì´ì§• í™˜ê²½ì—ì„œ A/B í…ŒìŠ¤íŠ¸")
                
        elif recommendation['action'] == "retrain_required":
            final_action = "schedule_retrain"
            print(f"ğŸ”„ ê²°ì •: ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì¬í›ˆë ¨ ìŠ¤ì¼€ì¤„")
        else:
            final_action = "manual_review"
            print(f"ğŸ‘¤ ê²°ì •: ìˆ˜ë™ ê²€í†  í•„ìš”")
        
        return {
            "action": final_action,
            "mlflow_recommendation": recommendation,
            "model_uri": mlflow_result['model_uri'],
            "model_stage": registration['stage'],
            "confidence": recommendation['confidence']
        }

    @task
    def send_notification(deployment_decision: Dict[str, Any], mlflow_result: Dict[str, Any], **context):
        """
        ìµœì¢… ë°°í¬ ê²°ì • ê²°ê³¼ë¥¼ ì•Œë¦¼ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
        """
        params = context['params']
        dataset = params['dataset']
        model_type = params['model_type']
        
        action = deployment_decision['action']
        confidence = deployment_decision['confidence']
        accuracy = mlflow_result['metrics']['accuracy']
        
        print(f"ğŸ“¢ MLOps íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ì•Œë¦¼")
        print(f"ğŸ“Š ë°ì´í„°ì…‹: {dataset}")
        print(f"ğŸ¤– ëª¨ë¸ íƒ€ì…: {model_type}")
        print(f"ğŸ¯ ìµœì¢… ì •í™•ë„: {accuracy:.4f}")
        print(f"ğŸš€ ë°°í¬ ê²°ì •: {action}")
        print(f"ğŸ’ª ì‹ ë¢°ë„: {confidence}")
        
        # ì•¡ì…˜ë³„ ë©”ì‹œì§€
        if action == "deploy_to_production":
            message = f"ğŸ‰ ìƒˆë¡œìš´ ê³ ì„±ëŠ¥ ëª¨ë¸ì´ í”„ë¡œë•ì…˜ì— ë°°í¬ë©ë‹ˆë‹¤! (ì •í™•ë„: {accuracy:.4f})"
        elif action == "deploy_to_staging":
            message = f"ğŸ“Š ìƒˆë¡œìš´ ëª¨ë¸ì´ ìŠ¤í…Œì´ì§•ì—ì„œ A/B í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!"
        elif action == "schedule_retrain":
            message = f"ğŸ”„ ëª¨ë¸ ì„±ëŠ¥ ë¶€ì¡±ìœ¼ë¡œ ì¬í›ˆë ¨ì´ ìŠ¤ì¼€ì¤„ë©ë‹ˆë‹¤."
        else:
            message = f"ğŸ‘¤ ëª¨ë¸ ê²°ê³¼ì— ëŒ€í•œ ìˆ˜ë™ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        
        print(f"ğŸ’Œ ì•Œë¦¼ ë©”ì‹œì§€: {message}")
        
        # ì‹¤ì œë¡œëŠ” Slack, Teams, Email ë“±ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡
        # slack_webhook = "https://hooks.slack.com/..."
        # requests.post(slack_webhook, json={
        #     "text": message,
        #     "attachments": [{
        #         "fields": [
        #             {"title": "Dataset", "value": dataset, "short": True},
        #             {"title": "Model", "value": model_type, "short": True},
        #             {"title": "Accuracy", "value": f"{accuracy:.4f}", "short": True},
        #             {"title": "Action", "value": action, "short": True}
        #         ]
        #     }]
        # })
        
        return {
            "notification_sent": True,
            "message": message,
            "recipients": ["ml-team@company.com", "devops@company.com"]
        }

    # Task ì˜ì¡´ì„± ì •ì˜ - í†µí•© MLflow ì ‘ê·¼ë²•
    validated = validate_parameters()
    dataset_loaded = load_dataset_from_minio(validated)
    features_engineered = feature_engineering(dataset_loaded)
    
    # MLflowê°€ í›ˆë ¨â†’í‰ê°€â†’ê²€ì¦â†’ë“±ë¡ì„ í†µí•©ìœ¼ë¡œ ìˆ˜í–‰
    mlflow_pipeline_result = run_complete_mlflow_pipeline(features_engineered)
    
    # Airflowê°€ MLflow ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°°í¬ ê²°ì •
    deployment_decision = decide_deployment_action(mlflow_pipeline_result)
    
    # ìµœì¢… ì•Œë¦¼
    notification = send_notification(deployment_decision, mlflow_pipeline_result)
    
    # ì˜ì¡´ì„± ì²´ì¸ - ë” ê°„ë‹¨í•´ì§!
    validated >> dataset_loaded >> features_engineered >> mlflow_pipeline_result >> deployment_decision >> notification

# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
mlops_training_pipeline()