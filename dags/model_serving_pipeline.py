"""
# ëª¨ë¸ ì„œë¹™ íŒŒì´í”„ë¼ì¸ - í”„ë¡œë•ì…˜ ë°°í¬ ë° ì„œë¹™ ê´€ë¦¬

## í”„ë¡œì íŠ¸ ì •ë³´
- **DAG ID**: model_serving_pipeline
- **ì‘ì„±ì**: eric.cho@robos.one
- **ì‘ì„±ì¼**: 2025-08-24
- **ëª©ì **: í›ˆë ¨ëœ ëª¨ë¸ì˜ í”„ë¡œë•ì…˜ ë°°í¬, ì„œë¹™, ëª¨ë‹ˆí„°ë§ ê´€ë¦¬

## ê°œìš”
í›ˆë ¨ íŒŒì´í”„ë¼ì¸ê³¼ ë¶„ë¦¬ëœ ë³„ë„ì˜ ì„œë¹™ ì „ìš© DAGë¡œ:
1. MLflow Model Registryì—ì„œ ìŠ¹ì¸ëœ ëª¨ë¸ ê°ì§€
2. ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë¹Œë“œ ë° ë°°í¬
3. API ì„œë²„ ë°°í¬ (FastAPI/Flask)
4. í—¬ìŠ¤ì²´í¬ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
5. Blue-Green ë°°í¬ ê´€ë¦¬

## íŠ¸ë¦¬ê±° ì¡°ê±´
- MLflow Model Registryì˜ ëª¨ë¸ì´ "Production" stageë¡œ ì „í™˜ë  ë•Œ
- ìˆ˜ë™ íŠ¸ë¦¬ê±° (ê¸´ê¸‰ ë°°í¬ ì‹œ)
- ì •ê¸°ì ì¸ ì„œë¹™ ìƒíƒœ ì²´í¬ (ë§¤ì‹œê°„)

## ì„œë¹™ ì•„í‚¤í…ì²˜
```
MLflow Registry â†’ Docker Build â†’ K8s Deploy â†’ Load Balancer â†’ Monitoring
                                     â†“
                            Health Check â†’ Rollback (í•„ìš”ì‹œ)
```
"""
import datetime
import pendulum
from typing import Dict, Any

from airflow.sdk import dag, task, Asset
from airflow.models.param import Param

# ëª¨ë¸ ì„œë¹™ ê´€ë ¨ Assets
production_model_asset = Asset(
    uri="mlflow://models/production_model",
    name="production_ready_model", 
    extra={"stage": "Production", "env": "prod"}
)

serving_endpoint_asset = Asset(
    uri="https://api.company.com/ml/predict",
    name="ml_serving_endpoint",
    extra={"type": "rest_api", "framework": "fastapi"}
)

@dag(
    dag_id="model_serving_pipeline",
    # ëª¨ë¸ì´ Production stageë¡œ ì „í™˜ë˜ë©´ ìë™ ì‹¤í–‰
    schedule=[production_model_asset],  
    start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
    catchup=False,
    tags=["mlops", "serving", "production", "kubernetes"],
    params={
        "deployment_strategy": Param(
            default="blue_green",
            type="string",
            enum=["blue_green", "rolling_update", "canary"],
            description="ë°°í¬ ì „ëµ ì„ íƒ"
        ),
        "replicas": Param(
            default=3,
            type="integer",
            description="ì„œë¹™ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜"
        ),
        "cpu_limit": Param(
            default="500m",
            type="string", 
            description="CPU ì œí•œ (Kubernetes)"
        ),
        "memory_limit": Param(
            default="1Gi",
            type="string",
            description="ë©”ëª¨ë¦¬ ì œí•œ (Kubernetes)"
        ),
        "auto_scaling": Param(
            default=True,
            type="boolean",
            description="ì˜¤í† ìŠ¤ì¼€ì¼ë§ í™œì„±í™”"
        )
    },
    default_args={
        "owner": "ml-platform-team@company.com",
        "retries": 2,
        "retry_delay": pendulum.duration(minutes=5),
    },
    description="í”„ë¡œë•ì…˜ ëª¨ë¸ ì„œë¹™ ë°°í¬ ë° ê´€ë¦¬ íŒŒì´í”„ë¼ì¸",
    doc_md=__doc__,
)
def model_serving_pipeline():

    @task(inlets=[production_model_asset])
    def fetch_production_model():
        """
        MLflow Model Registryì—ì„œ Production stage ëª¨ë¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        """
        print("ğŸ” MLflow Model Registry ì¡°íšŒ ì¤‘...")
        
        # ì‹¤ì œë¡œëŠ” MLflow Client ì‚¬ìš©
        # from mlflow.tracking import MlflowClient
        # client = MlflowClient()
        # models = client.search_model_versions("stage='Production'")
        
        # ì˜ˆì œìš© ë”ë¯¸ ë°ì´í„°
        production_models = [
            {
                "name": "iris_best_model",
                "version": "3",
                "model_uri": "s3://ml-bucket/models/iris_xgboost_v3",
                "stage": "Production",
                "description": "XGBoost classifier for iris dataset",
                "accuracy": 0.96,
                "created_timestamp": "2025-08-24T07:00:00Z",
                "tags": {"dataset": "iris", "model_type": "xgboost"}
            }
        ]
        
        if not production_models:
            print("âŒ Production stage ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            raise ValueError("No production models found")
        
        latest_model = production_models[0]  # ìµœì‹  ëª¨ë¸ ì„ íƒ
        
        print(f"âœ… Production ëª¨ë¸ ë°œê²¬:")
        print(f"   ğŸ“› ëª¨ë¸ëª…: {latest_model['name']}")
        print(f"   ğŸ”¢ ë²„ì „: {latest_model['version']}")
        print(f"   ğŸ¯ ì •í™•ë„: {latest_model['accuracy']}")
        print(f"   ğŸ“ ê²½ë¡œ: {latest_model['model_uri']}")
        
        return latest_model

    @task
    def create_bentoml_service(model_info: Dict[str, Any], **context):
        """
        BentoMLì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì„œë¹™ ì„œë¹„ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        model_name = model_info['name']
        model_version = model_info['version']
        model_uri = model_info['model_uri']
        model_type = model_info['tags'].get('model_type', 'sklearn')
        
        print(f"ğŸ± BentoML ì„œë¹„ìŠ¤ ìƒì„± ì‹œì‘")
        print(f"ğŸ“¦ ì„œë¹„ìŠ¤ëª…: {model_name}_service")
        print(f"ğŸ¤– ëª¨ë¸ íƒ€ì…: {model_type}")
        
        # BentoML ì„œë¹„ìŠ¤ ì •ì˜ íŒŒì¼ ìƒì„±
        bentofile_content = f"""
service: "service.py:svc"
labels:
  owner: ml-team
  stage: production
include:
  - "service.py"
  - "requirements.txt"
python:
  requirements_txt: requirements.txt
"""
        
        requirements_content = """
bentoml>=1.2.0
mlflow>=2.8.0
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
xgboost>=1.7.0
lightgbm>=3.3.0
"""

        # BentoML ì„œë¹„ìŠ¤ ì½”ë“œ ìƒì„±
        if model_type == "xgboost":
            service_content = f"""
import bentoml
import numpy as np
import pandas as pd
from typing import List

# MLflow ëª¨ë¸ì„ BentoML ëª¨ë¸ë¡œ ì„í¬íŠ¸
mlflow_model = bentoml.mlflow.import_model(
    "iris_xgboost_model",
    model_uri="{model_uri}",
    labels={{"version": "{model_version}"}},
)

@bentoml.service(
    resources={{"cpu": "500m", "memory": "1Gi"}},
    traffic={{"timeout": 20}},
)
class {model_name.title()}Service:
    
    bento_model = bentoml.models.get("iris_xgboost_model:latest")
    
    @bentoml.api
    def predict(self, input_data: List[List[float]]) -> List[int]:
        \"\"\"
        XGBoost ëª¨ë¸ ì˜ˆì¸¡ API
        \"\"\"
        df = pd.DataFrame(input_data)
        predictions = self.bento_model.predict(df)
        return predictions.tolist()
    
    @bentoml.api  
    def predict_proba(self, input_data: List[List[float]]) -> List[List[float]]:
        \"\"\"
        ì˜ˆì¸¡ í™•ë¥  ë°˜í™˜ API
        \"\"\"
        df = pd.DataFrame(input_data)
        probabilities = self.bento_model.predict_proba(df)
        return probabilities.tolist()
    
    @bentoml.api
    def batch_predict(self, batch_data: List[List[float]]) -> List[int]:
        \"\"\"
        ë°°ì¹˜ ì˜ˆì¸¡ API (ìë™ ë°°ì¹˜ ìµœì í™”)
        \"\"\"
        df = pd.DataFrame(batch_data)
        predictions = self.bento_model.predict(df)
        return predictions.tolist()

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
svc = {model_name.title()}Service()
"""
        else:  # sklearn or lightgbm
            service_content = f"""
import bentoml
import numpy as np
import pandas as pd
from typing import List, Dict

# MLflow ëª¨ë¸ì„ BentoML ëª¨ë¸ë¡œ ì„í¬íŠ¸
mlflow_model = bentoml.mlflow.import_model(
    "{model_name}_model",
    model_uri="{model_uri}",
    labels={{"version": "{model_version}", "framework": "{model_type}"}},
)

@bentoml.service(
    resources={{"cpu": "500m", "memory": "1Gi"}},
    traffic={{"timeout": 20}},
)
class {model_name.title()}Service:
    
    bento_model = bentoml.models.get("{model_name}_model:latest")
    
    @bentoml.api
    def predict(self, input_data: List[List[float]]) -> List[int]:
        \"\"\"
        ë‹¨ì¼/ë‹¤ì¤‘ ì˜ˆì¸¡ API
        ì…ë ¥: [[feature1, feature2, feature3, feature4], ...]
        ì¶œë ¥: [prediction1, prediction2, ...]
        \"\"\"
        df = pd.DataFrame(input_data)
        predictions = self.bento_model.predict(df)
        return predictions.tolist()
    
    @bentoml.api
    def predict_single(self, features: List[float]) -> Dict:
        \"\"\"
        ë‹¨ì¼ ìƒ˜í”Œ ì˜ˆì¸¡ (ìƒì„¸ ê²°ê³¼ í¬í•¨)
        \"\"\"
        df = pd.DataFrame([features])
        prediction = self.bento_model.predict(df)[0]
        
        if hasattr(self.bento_model, 'predict_proba'):
            probabilities = self.bento_model.predict_proba(df)[0]
            return {{
                "prediction": int(prediction),
                "probabilities": probabilities.tolist(),
                "confidence": float(max(probabilities))
            }}
        else:
            return {{
                "prediction": float(prediction) if hasattr(prediction, 'dtype') else prediction
            }}
    
    @bentoml.api
    def health_check(self) -> Dict:
        \"\"\"
        ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬
        \"\"\"
        return {{
            "status": "healthy",
            "model_name": "{model_name}",
            "model_version": "{model_version}",
            "framework": "{model_type}",
            "bento_version": bentoml.__version__
        }}

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤  
svc = {model_name.title()}Service()
"""
        
        print(f"ğŸ“„ BentoML ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±ì™„ë£Œ")
        print(f"   - bentofile.yaml")
        print(f"   - service.py") 
        print(f"   - requirements.txt")
        
        # BentoML ë¹Œë“œ ì‹œë®¬ë ˆì´ì…˜
        bento_tag = f"{model_name}_service:{model_version}"
        print(f"ğŸ”¨ Bento ë¹Œë“œ ì§„í–‰ ì¤‘...")
        print(f"   bentoml build")
        print(f"   bentoml containerize {bento_tag}")
        
        # ìë™ ìƒì„±ë˜ëŠ” íŠ¹ì§•ë“¤
        auto_features = [
            "ğŸŒ OpenAPI/Swagger UI ìë™ ìƒì„±",
            "ğŸ“Š Prometheus ë©”íŠ¸ë¦­ ë‚´ì¥",
            "ğŸ”„ ìë™ ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”", 
            "ğŸ¥ í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸",
            "ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§",
            "ğŸ”€ A/B í…ŒìŠ¤íŠ¸ ì§€ì›"
        ]
        
        print(f"âœ¨ BentoML ìë™ ê¸°ëŠ¥ë“¤:")
        for feature in auto_features:
            print(f"   {feature}")
        
        print(f"âœ… BentoML ì„œë¹„ìŠ¤ ìƒì„± ì™„ë£Œ!")
        
        return {
            "bento_tag": bento_tag,
            "service_name": f"{model_name}_service",
            "bentofile": bentofile_content,
            "service_code": service_content,
            "requirements": requirements_content,
            "auto_features": auto_features,
            "swagger_url": f"http://localhost:3000/docs",  # BentoML ê¸°ë³¸ í¬íŠ¸
            "metrics_url": f"http://localhost:3000/metrics"
        }

    @task 
    def deploy_bento_as_docker_container(model_info: Dict[str, Any], bento_info: Dict[str, Any], **context):
        """
        BentoML ì„œë¹„ìŠ¤ë¥¼ Docker ì»¨í…Œì´ë„ˆë¡œ ì§ì ‘ ë°°í¬í•©ë‹ˆë‹¤.
        """
        params = context['params']
        replicas = params['replicas']
        cpu_limit = params['cpu_limit']
        memory_limit = params['memory_limit']
        
        model_name = model_info['name']
        bento_tag = bento_info['bento_tag']
        service_name = bento_info['service_name']
        
        print(f"ğŸ³ BentoML Docker ì»¨í…Œì´ë„ˆ ë°°í¬ ì‹œì‘")
        print(f"ğŸ± Bento íƒœê·¸: {bento_tag}")
        print(f"ğŸ“Š ì»¨í…Œì´ë„ˆ ìˆ˜: {replicas}ê°œ")
        print(f"ğŸ’» ë¦¬ì†ŒìŠ¤: CPU={cpu_limit}, Memory={memory_limit}")
        
        # Docker Compose ì„¤ì • ìƒì„±
        docker_compose_content = f"""
version: '3.8'
services:
  {service_name}:
    image: {bento_tag}
    container_name: {service_name}-main
    ports:
      - "8080:3000"  # ì™¸ë¶€:ë‚´ë¶€ í¬íŠ¸ ë§¤í•‘
    environment:
      - BENTOML_PORT=3000
      - BENTOML_PRODUCTION=true
      - BENTOML_METRICS_ENABLED=true
    deploy:
      resources:
        limits:
          cpus: '{cpu_limit.replace("m", "").rstrip("0")}0'  # 500m -> 0.5
          memory: {memory_limit}
        reservations:
          cpus: '0.2'
          memory: 512M
    volumes:
      - bento_logs:/opt/bentoml/logs
      - bento_cache:/tmp/bentoml
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health_check"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  nginx:
    image: nginx:alpine
    container_name: {service_name}-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - nginx_logs:/var/log/nginx
    depends_on:
      - {service_name}
    restart: unless-stopped

volumes:
  bento_logs:
  bento_cache:
  nginx_logs:

networks:
  default:
    name: {service_name}-network
"""
        
        # Nginx ë¡œë“œë°¸ëŸ°ì„œ ì„¤ì • (ì—¬ëŸ¬ ì»¨í…Œì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤ìš©)
        nginx_config = f"""
events {{
    worker_connections 1024;
}}

http {{
    upstream bentoml_backend {{
        # ì—¬ëŸ¬ BentoML ì»¨í…Œì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤
        server {service_name}:3000;
        # server {service_name}-2:3000;  # í•„ìš”ì‹œ ì¶”ê°€
        # server {service_name}-3:3000;
    }}

    # ë¡œê·¸ í˜•ì‹
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                   '$status $body_bytes_sent "$http_referer" '
                   '"$http_user_agent" "$http_x_forwarded_for"';

    server {{
        listen 80;
        server_name localhost;
        
        # ì•¡ì„¸ìŠ¤ ë¡œê·¸
        access_log /var/log/nginx/access.log main;
        error_log /var/log/nginx/error.log;

        # ì˜ˆì¸¡ API
        location /predict {{
            proxy_pass http://bentoml_backend/predict;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }}

        # í—¬ìŠ¤ì²´í¬
        location /health {{
            proxy_pass http://bentoml_backend/health_check;
            proxy_set_header Host $host;
        }}

        # Swagger UI
        location /docs {{
            proxy_pass http://bentoml_backend/docs;
            proxy_set_header Host $host;
        }}

        # Prometheus ë©”íŠ¸ë¦­
        location /metrics {{
            proxy_pass http://bentoml_backend/metrics;
            proxy_set_header Host $host;
        }}

        # ëª¨ë“  API ê²½ë¡œ
        location / {{
            proxy_pass http://bentoml_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }}
    }}
}}
"""

        print("ğŸ“„ Docker ë°°í¬ ì„¤ì • ìƒì„±ì™„ë£Œ:")
        print("   âœ… docker-compose.yml")
        print("   âœ… nginx.conf (ë¡œë“œë°¸ëŸ°ì„œ)")
        
        # ì‹¤ì œ Docker ëª…ë ¹ì–´ë“¤
        docker_commands = [
            f"# BentoML ì´ë¯¸ì§€ ë¹Œë“œ",
            f"bentoml build",
            f"bentoml containerize {bento_tag}",
            "",
            f"# Docker Composeë¡œ ì„œë¹„ìŠ¤ ì‹œì‘", 
            f"docker-compose up -d",
            "",
            f"# ë˜ëŠ” ê°œë³„ ì»¨í…Œì´ë„ˆ ì‹¤í–‰",
            f"docker run -d \\",
            f"  --name {service_name}-1 \\",
            f"  --restart unless-stopped \\",
            f"  -p 8080:3000 \\",
            f"  -e BENTOML_PORT=3000 \\",
            f"  -e BENTOML_PRODUCTION=true \\",
            f"  --cpus='{cpu_limit.replace('m', '').rstrip('0')}0' \\",
            f"  --memory={memory_limit} \\",
            f"  -v bento_logs:/opt/bentoml/logs \\",
            f"  --health-cmd='curl -f http://localhost:3000/health_check' \\",
            f"  --health-interval=30s \\",
            f"  --health-timeout=10s \\",
            f"  --health-retries=3 \\",
            f"  {bento_tag}",
        ]
        
        print(f"\nğŸ³ Docker ë°°í¬ ëª…ë ¹ì–´:")
        for cmd in docker_commands:
            print(f"   {cmd}")
        
        # ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰ (ìŠ¤ì¼€ì¼ë§)
        if replicas > 1:
            print(f"\nğŸ“Š ìŠ¤ì¼€ì¼ë§: {replicas}ê°œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
            for i in range(1, replicas + 1):
                port = 8080 + (i - 1)
                scale_cmd = f"docker run -d --name {service_name}-{i} -p {port}:3000 {bento_tag}"
                print(f"   {scale_cmd}")

        # ì»¨í…Œì´ë„ˆ ê´€ë¦¬ ëª…ë ¹ì–´ë“¤
        management_commands = {
            "ìƒíƒœ í™•ì¸": f"docker ps -f name={service_name}",
            "ë¡œê·¸ í™•ì¸": f"docker logs {service_name}-main",
            "í—¬ìŠ¤ì²´í¬": f"curl http://localhost:8080/health_check",
            "ì¤‘ì§€": f"docker-compose down",
            "ì¬ì‹œì‘": f"docker-compose restart",
            "ìŠ¤ì¼€ì¼ë§": f"docker-compose up -d --scale {service_name}={replicas}",
            "ì •ë¦¬": f"docker system prune -f"
        }
        
        print(f"\nğŸ› ï¸ ì»¨í…Œì´ë„ˆ ê´€ë¦¬ ëª…ë ¹ì–´:")
        for desc, cmd in management_commands.items():
            print(f"   {desc}: {cmd}")
        
        print(f"\nâœ… BentoML Docker ì»¨í…Œì´ë„ˆ ë°°í¬ ì™„ë£Œ!")
        
        # ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ ì •ë³´
        endpoints = {
            "main_api": "http://localhost:8080",
            "predict": "http://localhost:8080/predict",
            "predict_single": "http://localhost:8080/predict_single", 
            "batch_predict": "http://localhost:8080/batch_predict",
            "health_check": "http://localhost:8080/health_check",
            "docs": "http://localhost:8080/docs",  # Swagger UI
            "metrics": "http://localhost:8080/metrics",  # Prometheus
            "nginx_status": "http://localhost:80"  # Nginxë¥¼ í†µí•œ ì•¡ì„¸ìŠ¤
        }
        
        return {
            "container_name": f"{service_name}-main",
            "service_name": service_name,
            "replicas": replicas,
            "docker_compose": docker_compose_content,
            "nginx_config": nginx_config,
            "endpoints": endpoints,
            "management_commands": management_commands,
            "bento_tag": bento_tag,
            "ports": {
                "main": 8080,
                "nginx": 80,
                "https": 443
            }
        }

    @task(outlets=[serving_endpoint_asset])
    def setup_monitoring_and_alerts(model_info: Dict[str, Any], k8s_info: Dict[str, Any]):
        """
        ì„œë¹™ ëª¨ë¸ì˜ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        model_name = model_info['name']
        service_name = k8s_info['service_name']
        
        print(f"ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„¤ì • ì‹œì‘")
        print(f"ğŸ” ëŒ€ìƒ ì„œë¹„ìŠ¤: {service_name}")
        
        # Prometheus ë©”íŠ¸ë¦­ ì„¤ì •
        monitoring_config = {
            "metrics": [
                "http_requests_total",
                "http_request_duration_seconds", 
                "ml_prediction_latency",
                "ml_prediction_accuracy",
                "kubernetes_pod_cpu_usage",
                "kubernetes_pod_memory_usage"
            ],
            "alerts": [
                {
                    "name": "HighErrorRate",
                    "condition": "error_rate > 0.05",
                    "severity": "warning"
                },
                {
                    "name": "HighLatency", 
                    "condition": "p95_latency > 1000ms",
                    "severity": "critical"
                },
                {
                    "name": "ModelDrift",
                    "condition": "prediction_drift > 0.1",
                    "severity": "warning"
                }
            ]
        }
        
        print(f"ğŸ“ˆ Prometheus ë©”íŠ¸ë¦­ ì„¤ì •:")
        for metric in monitoring_config['metrics']:
            print(f"   - {metric}")
            
        print(f"ğŸš¨ ì•Œë¦¼ ê·œì¹™ ì„¤ì •:")
        for alert in monitoring_config['alerts']:
            print(f"   - {alert['name']}: {alert['condition']} ({alert['severity']})")
        
        # Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •
        dashboard_config = {
            "title": f"ML Serving - {model_name}",
            "panels": [
                "Request Rate",
                "Response Time",
                "Error Rate", 
                "Prediction Distribution",
                "Resource Usage",
                "Model Performance"
            ]
        }
        
        print(f"ğŸ“Š Grafana ëŒ€ì‹œë³´ë“œ ìƒì„±: {dashboard_config['title']}")
        
        # ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸ ì •ë³´
        endpoint_url = f"https://api.company.com/ml/{model_name}/predict"
        
        print(f"âœ… ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ!")
        print(f"ğŸŒ ì„œë¹™ ì—”ë“œí¬ì¸íŠ¸: {endpoint_url}")
        
        return {
            "endpoint_url": endpoint_url,
            "monitoring_enabled": True,
            "dashboard_url": f"https://grafana.company.com/d/ml-serving-{model_name}",
            "alerts_configured": len(monitoring_config['alerts'])
        }

    @task
    def run_health_checks(model_info: Dict[str, Any], monitoring_info: Dict[str, Any]):
        """
        ë°°í¬ëœ ëª¨ë¸ì˜ í—¬ìŠ¤ì²´í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        model_name = model_info['name']
        endpoint_url = monitoring_info['endpoint_url']
        
        print(f"ğŸ¥ í—¬ìŠ¤ì²´í¬ ì‹œì‘")
        print(f"ğŸ¯ ëŒ€ìƒ ì—”ë“œí¬ì¸íŠ¸: {endpoint_url}")
        
        health_checks = [
            {"name": "API ì‘ë‹µì„±", "status": "OK", "latency": "45ms"},
            {"name": "ëª¨ë¸ ë¡œë”©", "status": "OK", "memory": "312MB"}, 
            {"name": "ì˜ˆì¸¡ ì •í™•ì„±", "status": "OK", "accuracy": "96.2%"},
            {"name": "ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰", "status": "OK", "cpu": "23%"},
            {"name": "ë¡œê·¸ ìˆ˜ì§‘", "status": "OK", "rate": "100/min"}
        ]
        
        print(f"ğŸ” í—¬ìŠ¤ì²´í¬ ê²°ê³¼:")
        all_healthy = True
        for check in health_checks:
            status_emoji = "âœ…" if check['status'] == 'OK' else "âŒ"
            print(f"   {status_emoji} {check['name']}: {check['status']}")
            if check['status'] != 'OK':
                all_healthy = False
        
        if all_healthy:
            print(f"ğŸ‰ ëª¨ë“  í—¬ìŠ¤ì²´í¬ í†µê³¼! ì„œë¹™ ì¤€ë¹„ ì™„ë£Œ!")
            deployment_status = "HEALTHY"
        else:
            print(f"âš ï¸ ì¼ë¶€ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨. ê²€í†  í•„ìš”.")
            deployment_status = "UNHEALTHY"
        
        return {
            "overall_status": deployment_status,
            "health_checks": health_checks,
            "endpoint_ready": all_healthy,
            "last_check_time": pendulum.now().isoformat()
        }

    @task
    def send_deployment_notification(
        model_info: Dict[str, Any], 
        k8s_info: Dict[str, Any], 
        monitoring_info: Dict[str, Any],
        health_status: Dict[str, Any]
    ):
        """
        ë°°í¬ ì™„ë£Œ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.
        """
        model_name = model_info['name']
        model_version = model_info['version']
        endpoint_url = monitoring_info['endpoint_url']
        dashboard_url = monitoring_info['dashboard_url']
        status = health_status['overall_status']
        
        print(f"ğŸ“¢ ë°°í¬ ì™„ë£Œ ì•Œë¦¼ ë°œì†¡")
        
        if status == "HEALTHY":
            message = f"""
ğŸ‰ ëª¨ë¸ ì„œë¹™ ë°°í¬ ì„±ê³µ!

ğŸ“Š ëª¨ë¸ ì •ë³´:
   - ëª¨ë¸ëª…: {model_name}
   - ë²„ì „: {model_version}  
   - ì •í™•ë„: {model_info['accuracy']:.2%}

ğŸš€ ë°°í¬ ì •ë³´:
   - ì—”ë“œí¬ì¸íŠ¸: {endpoint_url}
   - ë ˆí”Œë¦¬ì¹´: {k8s_info['replicas']}ê°œ
   - ì˜¤í† ìŠ¤ì¼€ì¼ë§: {'í™œì„±í™”' if k8s_info['auto_scaling_enabled'] else 'ë¹„í™œì„±í™”'}

ğŸ“Š ëª¨ë‹ˆí„°ë§:
   - ëŒ€ì‹œë³´ë“œ: {dashboard_url}
   - ì•Œë¦¼ ê·œì¹™: {monitoring_info['alerts_configured']}ê°œ ì„¤ì •ë¨

âœ… ëª¨ë“  í—¬ìŠ¤ì²´í¬ í†µê³¼ - ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!
"""
        else:
            message = f"""
âš ï¸ ëª¨ë¸ ì„œë¹™ ë°°í¬ ì™„ë£Œ (ì£¼ì˜ í•„ìš”)

ëª¨ë¸: {model_name} v{model_version}
ìƒíƒœ: {status}
ì—”ë“œí¬ì¸íŠ¸: {endpoint_url}

ì¼ë¶€ í—¬ìŠ¤ì²´í¬ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.
"""
        
        print(message)
        
        # ì‹¤ì œë¡œëŠ” Slack, Teams, PagerDuty ë“±ìœ¼ë¡œ ì•Œë¦¼
        # slack_webhook = "https://hooks.slack.com/..."
        # requests.post(slack_webhook, json={"text": message})
        
        return {
            "notification_sent": True,
            "message": message,
            "recipients": ["ml-team@company.com", "platform-team@company.com"],
            "deployment_successful": status == "HEALTHY"
        }

    # Task ì˜ì¡´ì„± ì •ì˜ - BentoML Docker ì„œë¹™ íŒŒì´í”„ë¼ì¸
    model_fetched = fetch_production_model()
    bento_service_created = create_bentoml_service(model_fetched)
    docker_deployed = deploy_bento_as_docker_container(model_fetched, bento_service_created)
    monitoring_setup = setup_monitoring_and_alerts(model_fetched, docker_deployed)
    health_checked = run_health_checks(model_fetched, monitoring_setup)
    notification_sent = send_deployment_notification(
        model_fetched, docker_deployed, monitoring_setup, health_checked
    )
    
    # ì˜ì¡´ì„± ì²´ì¸ - Docker ì»¨í…Œì´ë„ˆ ë°°í¬ ë°©ì‹
    model_fetched >> bento_service_created >> docker_deployed >> monitoring_setup >> health_checked >> notification_sent

# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model_serving_pipeline()